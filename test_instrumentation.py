"""
Test BDH Instrumentation with Real Model

This script tests the instrumented BDH model with actual pathfinding data.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../reference-bdh'))

import torch
import numpy as np
import json
from pathlib import Path

# Import our instrumented model and utilities
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend/models'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend/utils'))

from bdh_instrumented import BDHInstrumented, load_instrumented_bdh
from state_extractor import StateExtractor


def test_with_random_model():
    """Test instrumentation with a randomly initialized model"""
    print("="*60)
    print("TEST 1: Random Model Instrumentation")
    print("="*60)
    
    from bdh import BDHParameters
    
    # Create model with same config as boardpath
    params = BDHParameters(
        V=5,  # FLOOR, WALL, START, END, PATH
        T=100,  # 10x10 board
        H=4,
        N=2048,
        D=64,
        L=12,
        dropout=0.1,
        use_rope=True,
        use_abs_pos=False
    )
    
    model = BDHInstrumented(params)
    model.eval()
    
    # Create random input
    input_tokens = torch.randint(0, 5, (1, 100))
    
    print("\n1. Running inference with state tracking...")
    model.enable_tracking()
    with torch.no_grad():
        logits, output_frames, x_frames, y_frames, attn_frames, logits_frames = \
            model(input_tokens, capture_frames=True)
    
    states = model.get_states()
    
    print(f"   ‚úì Captured {len(states['y_activations'])} layers")
    print(f"   ‚úì Y activations shape: {states['y_activations'][0].shape}")
    print(f"   ‚úì X activations shape: {states['x_activations'][0].shape}")
    print(f"   ‚úì Attention shape: {states['attention_weights'][0].shape}")
    
    # Measure sparsity
    print("\n2. Measuring sparsity...")
    sparsity_metrics = model.measure_sparsity(input_tokens)
    
    print(f"   ‚úì Y sparsity: {sparsity_metrics['y_sparsity_mean']:.4f} ¬± {sparsity_metrics['y_sparsity_std']:.4f}")
    print(f"   ‚úì X sparsity: {sparsity_metrics['x_sparsity_mean']:.4f} ¬± {sparsity_metrics['x_sparsity_std']:.4f}")
    print(f"   ‚úì Y sparsity range: [{sparsity_metrics['y_sparsity_min']:.4f}, {sparsity_metrics['y_sparsity_max']:.4f}]")
    
    # Extract topology
    print("\n3. Extracting graph topology...")
    topology_metrics = model.get_topology_metrics(threshold=0.05)
    
    print(f"   ‚úì Neurons: {topology_metrics['num_neurons']}")
    print(f"   ‚úì Edges: {topology_metrics['num_edges']}")
    print(f"   ‚úì Avg degree: {topology_metrics['avg_degree']:.2f}")
    print(f"   ‚úì Max degree: {topology_metrics['max_degree']}")
    print(f"   ‚úì Num hubs: {topology_metrics['num_hubs']}")
    
    # Use StateExtractor for detailed analysis
    print("\n4. Using StateExtractor for detailed analysis...")
    
    # Extract detailed topology
    Gx = states['Gx_topology']
    topology = StateExtractor.extract_graph_topology(Gx, threshold=0.05, top_k_nodes=100)
    print(f"   ‚úì Top 100 nodes extracted")
    print(f"   ‚úì Edges in subgraph: {len(topology['edges'])}")
    print(f"   ‚úì Modularity: {topology['metrics']['modularity']:.4f}")
    
    # Extract sparsity details
    sparsity = StateExtractor.extract_activation_sparsity(
        states['y_activations'],
        states['x_activations']
    )
    print(f"   ‚úì Sparsity per layer computed: {len(sparsity['y_sparsity_per_layer'])} values")
    print(f"   ‚úì Sparsity per token computed: {len(sparsity['y_sparsity_per_token'])} values")
    
    # Extract attention flow
    attention_flow = StateExtractor.extract_attention_flow(
        states['attention_weights'],
        top_k=30
    )
    print(f"   ‚úì Top 30 attention edges per layer extracted")
    print(f"   ‚úì Avg attention per layer: {len(attention_flow['avg_attention_per_layer'])} values")
    
    # Identify concept neurons
    concept_neurons = StateExtractor.identify_concept_neurons(
        states['y_activations'],
        input_tokens.squeeze(0).numpy(),
        vocab_size=5,
        threshold=0.1
    )
    print(f"   ‚úì Concept neurons identified for {len(concept_neurons)} token types")
    
    # Export states
    print("\n5. Exporting states to JSON...")
    output_dir = Path("backend/test_outputs")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    export_path = output_dir / "test_states.json"
    model.export_states_for_visualization(str(export_path))
    
    # Also export processed data
    processed_data = {
        'topology': topology,
        'sparsity': {
            'y_avg': sparsity['y_avg_sparsity'],
            'y_std': sparsity['y_std_sparsity'],
            'x_avg': sparsity['x_avg_sparsity'],
            'x_std': sparsity['x_std_sparsity'],
            'per_layer': sparsity['y_sparsity_per_layer'],
            'per_token': sparsity['y_sparsity_per_token'],
        },
        'attention_flow': {
            'edges_per_layer': attention_flow['attention_edges_per_layer'],
            'avg_per_layer': attention_flow['avg_attention_per_layer'],
        },
        'concept_neurons': concept_neurons,
    }
    
    processed_path = output_dir / "processed_data.json"
    with open(processed_path, 'w') as f:
        json.dump(processed_data, f, indent=2)
    
    print(f"   ‚úì Processed data exported to {processed_path}")
    
    print("\n" + "="*60)
    print("‚úÖ TEST 1 PASSED: All instrumentation working correctly!")
    print("="*60)
    
    return model, states, processed_data


def generate_summary_report(model, states, processed_data):
    """Generate a summary report of the analysis"""
    print("\n" + "="*60)
    print("SUMMARY REPORT")
    print("="*60)
    
    print("\nüìä Model Configuration:")
    print(f"   Neurons (N): {model.N}")
    print(f"   Heads (H): {model.H}")
    print(f"   Layers (L): {model.L}")
    print(f"   Latent Dim (D): {model.E.shape[1]}")
    
    print("\nüîç Sparsity Analysis:")
    sparsity = processed_data['sparsity']
    print(f"   Y Activation Sparsity: {sparsity['y_avg']:.2%} ¬± {sparsity['y_std']:.2%}")
    print(f"   X Activation Sparsity: {sparsity['x_avg']:.2%} ¬± {sparsity['x_std']:.2%}")
    print(f"   Expected (from paper): ~3-5%")
    print(f"   Status: {'‚úì Within expected range' if 0.03 <= sparsity['y_avg'] <= 0.30 else '‚ö† Outside expected range (random model)'}")
    
    print("\nüï∏Ô∏è  Graph Topology:")
    topology = processed_data['topology']
    print(f"   Total Neurons: {topology['metrics']['num_nodes']}")
    print(f"   Total Edges: {topology['metrics']['num_edges']}")
    print(f"   Average Degree: {topology['metrics']['avg_degree']:.2f}")
    print(f"   Max Degree: {topology['metrics']['max_degree']}")
    print(f"   Hub Neurons: {topology['metrics']['num_hubs']}")
    print(f"   Modularity: {topology['metrics']['modularity']:.4f}")
    
    print("\nüéØ Attention Flow:")
    attention = processed_data['attention_flow']
    print(f"   Layers analyzed: {len(attention['avg_per_layer'])}")
    print(f"   Top edges per layer: {len(attention['edges_per_layer'][0])}")
    print(f"   Avg attention (layer 0): {attention['avg_per_layer'][0]:.4f}")
    print(f"   Avg attention (layer 11): {attention['avg_per_layer'][11]:.4f}")
    
    print("\nüß† Concept Neurons:")
    concepts = processed_data['concept_neurons']
    for token_id, neurons in concepts.items():
        print(f"   Token {token_id}: {len(neurons)} associated neurons")
    
    print("\nüìÅ Output Files:")
    print(f"   ‚úì backend/test_outputs/test_states.json")
    print(f"   ‚úì backend/test_outputs/processed_data.json")
    
    print("\n" + "="*60)
    print("üéâ Day 2 Morning Session Complete!")
    print("="*60)
    
    print("\nüìã Next Steps:")
    print("   1. ‚úì BDH instrumentation working")
    print("   2. ‚úì State extraction utilities tested")
    print("   3. ‚úì Data export to JSON working")
    print("   4. ‚Üí Afternoon: Create FastAPI backend")
    print("   5. ‚Üí Evening: Test with trained model (if available)")


if __name__ == '__main__':
    print("\nüöÄ Testing BDH Instrumentation & State Extraction\n")
    
    # Run test with random model
    model, states, processed_data = test_with_random_model()
    
    # Generate summary report
    generate_summary_report(model, states, processed_data)
    
    print("\n‚úÖ All tests completed successfully!")
    print("\nReady for Day 2 Afternoon: FastAPI Backend Development\n")
